from transformers import AutoTokenizer, AutoModel
import torch
import os
import pandas as pd
import numpy as np
import sys


autoGeneratedKeywords = ['this class was autogenerated', 'autogenerated', 'THIS FILE IS AUTOMATICALLY GENERATED', 'This code was generated by a tool', 'This source code was auto-generated by', 'automatically generated', 'This source code was auto-generated by wsdl', 'his class was automatically generated by a Snowball to Java compiler', 'This source code was auto-generated by MonoXSD','Generated.*by.*JFlex','generated by JFlex','The following code was generated by JFlex', 'generated by CUP', 'The following code was generated by CUP', 'Generated by Doxygen', 'generated by JavaCC', 'Generated by JavaCC: ignore naming convention violation', 'DO NOT EDIT THIS FILE - it is machine generated', 'Generated by Maven', 'Autogenerated by Thrift', 'Generated By:javaCC: Do not edit this line', 'This file was generated by SableCC', 'GeneratedOrderBy ANTLR']
# selected_repos = ['tapestry4', 'apr', 'etch', 'stdcxx', 'xalan-j', 'zookeeper', 'sling-old-svn-mirror', 'apr-iconv', 'tapestry3']



#### critic repos
critic_repos = ['camel-quarkus','tapestry3','tapestry4','etch','camel']
auto_repos = ["commons-compress", "incubator-streampark", "incubator-heron", "accumulo-proxy", "myfaces-build-tools", "activemq", "calcite", "dolphinscheduler", "openwhisk-intellij-plugin", "harmony-classlib", "empire-db", "ode", "sling-org-apache-sling-pipes", "maven-site-plugin", "sis", "samza", "qpid-broker-j", "netbeans", "ant", "incubator-linkis", "harmony", "sling-org-apache-sling-servlets-post", "shiro-site", "kafka", "phoenix-tephra", "plc4x", "tapestry3", "jclouds-labs-aws", "sling-org-apache-sling-jcr-jackrabbit-usermanager", "asterixdb-graph", "ignite-3", "pig", "oozie", "hbase", "nifi", "flink-benchmarks", "incubator-retired-htrace", "rya", "photark-mobile", "tapestry4", "flink-statefun", "wink", "jmeter", "metamodel", "apex-malhar", "xalan-java", "rocketmq", "maven", "commons-cli", "camel-quarkus", "ignite", "tomee", "maven-sandbox", "tomcat", "groovy", "royale-compiler", "arrow", "qpid-jms", "drill", "flex-sdk", "geronimo-xbean", "derby", "stanbol", "tuscany-sca-2.x", "cassandra", "tiles-autotag", "geronimo-devtools", "tuscany-sca-1.x", "uima-uimafit", "ambari", "flex-whiteboard", "flink-connector-cassandra", "poi", "geode", "asterixdb-clients", "freemarker", "flume", "hive", "incubator-retired-wave", "juddi", "airavata-sandbox", "metron", "directmemory-lightning", "turbine-core", "incubator-retired-blur", "flex-blazeds", "brooklyn-server", "wicket", "subversion", "incubator-sentry", "zeppelin", "hcatalog", "maven-resolver", "ignite-extensions", "sentry", "cocoon", "qpid-jms-amqp-0-x", "openejb", "jackrabbit-oak", "distributedlog", "airavata", "camel", "dubbo-samples", "jena", "maven-plugins", "pulsar-connectors", "jackrabbit-filevault-package-maven-plugin", "nifi-minifi", "commons-scxml", "shiro", "xmlgraphics-commons", "doris", "storm", "asterixdb", "logging-log4j-tools", "pulsar", "beam", "odftoolkit", "fluo", "maven-doap-plugin", "xmlgraphics-fop", "avro", "turbine-fulcrum-security", "lucene", "maven-ant-plugin", "turbine-archetypes", "jclouds", "opennlp", "cayenne", "ctakes", "commons-digester", "directory-fortress-core", "cxf", "creadur-rat", "incubator-brooklyn", "incubator-pegasus", "bookkeeper", "james-project", "openoffice", "uima-uimaj", "polygene-java", "axis-axis1-java", "solr", "activemq-cpp", "uima-addons", "velocity-engine", "myfaces-trinidad-maven", "mina", "commons-jxpath", "pinot", "felix-dev", "aries", "tomcat-taglibs-standard", "asterixdb-hyracks", "incubator-retired-openaz", "freemarker-docgen", "xmlbeans", "geronimo-yoko", "dubbo-spi-extensions", "accumulo", "maven-help-plugin", "flink", "etch"]


def batch(iterable, n=1):
    l = len(iterable)
    for ndx in range(0, l, n):
        yield iterable[ndx:min(ndx + n, l)]


# for i in range(0,len(code_tokens),510):
#     print(c)
#     c += 1
#     if i+510 < len(code_tokens):
#         tokens = [tokenizer.cls_token]+code_tokens[i:i+510]+[tokenizer.sep_token]
#     else:
#         tokens = [tokenizer.cls_token]+code_tokens[i:]+[tokenizer.sep_token]


# Init
tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModel.from_pretrained("microsoft/codebert-base")

# savePath = 'appacheCodebertDataframes'
# # savePath = 'apacheAutoDataset'
# savePath = 'selectedReposDataframe'

# new
savePath = 'mainDatasets/auto2'
# savePath = 'mainDatasets/humanDataframes'

# Tokenization 
sourceDirPath="ApacheRepos"

repos = os.listdir(sourceDirPath)
print(len(repos))
trackedFiles = os.listdir(savePath)
repo_count = 0
for repo in auto_repos:
    print(f'{repo_count}--------------{repo}---------------')
    repo_count += 1
    checked = 0
    numberOfAutoGenerated = 0
    # check if a file is tracked before
    # for tracedFileName in trackedFiles:
    #     if repo in tracedFileName:
    #         print(repo, 'processed before.')
    #         checked = 1
    #         break
    # if checked:
    #     continue

    
    print(f'repo {repo} started ...')
    counter = 0
    itr = 0
    dataset = []
    dataset1 = []
    dataset2 = []

    sourceRepoPath = sourceDirPath + '/' + repo

    for root, dirs, files in os.walk(sourceRepoPath):
        for name in files:
            path = os.path.join(root, name)
            if '.java' in path:
                counter +=1
                if counter % 50 == 0:
                    print('counter:', counter)
                try:
                    with open(path, 'r') as openfile:
                        content = openfile.read()
                        
                        # set label using keywords
                        label=0
                        ## TODO: for get label please uncomment below lines

                        for keyword in autoGeneratedKeywords:
                            if keyword in content:
                                label = 1
                                print('Do not touch me. I am Autogenerated java file')
                                break # keyword is found
                        if label == 0:
                            continue

                        code_tokens = tokenizer.tokenize(content)

                        del content

                        # batch_context_embeddings = []
                        # # for batch_code_tokens in batch(code_tokens, 510):
                        # for i in range(0,len(code_tokens),510):
                        #     if i+510 < len(code_tokens):
                        #         tokens = [tokenizer.cls_token]+code_tokens[i:i+510]+[tokenizer.sep_token]
                        #     else:
                        #         tokens = [tokenizer.cls_token]+code_tokens[i:]+[tokenizer.sep_token]
                            
                        #     tokens_ids = tokenizer.convert_tokens_to_ids(tokens)
                        #     del tokens
                            
                        #     context_embeddings = model(torch.tensor(tokens_ids)[None,:])[0]
                        #     context_embeddings = context_embeddings.mean(axis=(1))
                        #     batch_context_embeddings.append(context_embeddings)
                        
                        # mean_context_embeddings = torch.stack(batch_context_embeddings).mean(axis=(0))
                        # del batch_context_embeddings

                        #### NEW VERSION ####
                        batch_context_embeddings = []
                        numOfBatchs = int(len(code_tokens)/510)+1
                        mean_context_embeddings = torch.zeros(1,768)
                        firstTime = 0
                        for batch_code_tokens in batch(code_tokens, 510):
                            tokens = [tokenizer.cls_token]+batch_code_tokens+[tokenizer.sep_token]
                            tokens_ids = tokenizer.convert_tokens_to_ids(tokens)
                            context_embeddings = model(torch.tensor(tokens_ids)[None,:])[0]
                            context_embeddings = context_embeddings.mean(axis=(1))
                            # batch_context_embeddings.append(context_embeddings)
                            
                            if firstTime:
                                mean_context_embeddings.add(torch.div(context_embeddings,numOfBatchs))
                            else:
                                mean_context_embeddings = torch.div(context_embeddings,numOfBatchs)
                                firstTime = 1
                            
                    
                        # mean_context_embeddings = torch.stack(batch_context_embeddings).mean(axis=(0))


                        dataset.append(mean_context_embeddings)
                        dataset1.append(label)
                        dataset2.append(path)

                                
                        if counter >= 50:
                            # create dataframe and save to disk for each repo
                            if len(dataset) == 0:
                                del dataset,dataset1,dataset2
                                print('******')
                                continue

                            temp_dataset = torch.stack(dataset).detach().numpy()
                            temp_dataset = temp_dataset.reshape(temp_dataset.shape[0],768)
                            
                            print(temp_dataset.shape)

                            df = pd.DataFrame(temp_dataset) # vector (embedding codebert) 768
                            df1 = pd.DataFrame(dataset1) # label           
                            df2 = pd.DataFrame(dataset2) # path          


                            # Merge dataframes 
                            df3 = pd.merge(df2,df,left_index=True, right_index=True)
                            df4 = pd.merge(df3,df1,left_index=True, right_index=True)
                            df4 = df4.rename(columns={'0_x': 'path', '0_y': '0',0: 'label'})

                            # save to disk
                            # savePath = "appacheCodebertDataframes"
                            # savePaht = 'apacheAutoDataset'

                            df4.to_csv(f'{savePath}/{repo}_{itr}.csv')
                            print(f'{repo}_{itr} dataset saved!')
                            itr += 1

                            # free memory
                            del df,df1,df2,df3,dataset,dataset1,dataset2
                            counter = 0
                            dataset = []
                            dataset1 = []
                            dataset2 = []
                            
                except:
                    print('error while reading java file')

    # create dataframe and save to disk for each repo
    if len(dataset) == 0:
        del dataset,dataset1,dataset2
        print('******')
        continue

    temp_dataset = torch.stack(dataset).detach().numpy()
    temp_dataset = temp_dataset.reshape(temp_dataset.shape[0],768)
    
    print(temp_dataset.shape)

    df = pd.DataFrame(temp_dataset) # vector (embedding codebert) 768
    df1 = pd.DataFrame(dataset1) # label           
    df2 = pd.DataFrame(dataset2) # path          


    # Merge dataframes 
    df3 = pd.merge(df2,df,left_index=True, right_index=True)
    df4 = pd.merge(df3,df1,left_index=True, right_index=True)
    df4 = df4.rename(columns={'0_x': 'path', '0_y': '0',0: 'label'})

    # save to disk
    # savePath = "appacheCodebertDataframes"
    # savePaht = 'apacheAutoDataset'

    df4.to_csv(f'{savePath}/{repo}_{itr}.csv')
    print(f'{repo} dataset saved!')

    # free memory
    del df,df1,df2,df3,dataset,dataset1,dataset2



# # read all saved repo dataframes and append together for training and processing
# sourceDfPath = "appacheCodebertDataframes"

# reposDFs = os.listdir(sourceDfPath)

# flag = 0
# # del df,df1
# dfs = []
# for repoDF in reposDFs:
#     sourceRepoDF = sourceDfPath + '/' + repoDF
#     if flag == 0:
#         df = pd.read_csv(sourceRepoDF)
#         flag = 1
#         continue
#     if flag == 1:
#         df1 = pd.read_csv(sourceRepoDF)
#         flag = 2
#         continue
#     if flag == 2:
#         df = pd.concat([df,df1], ignore_index=True)
#         df1 = pd.read_csv(sourceRepoDF)
# df = pd.concat([df,df1], ignore_index=True)
# df = df.iloc[:,1:]
# del df1 

# df.to_csv('merged_labeled_dataset.csv')


